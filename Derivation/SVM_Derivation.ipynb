{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a4ef870",
   "metadata": {},
   "source": [
    "# SVM Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0702b2ad",
   "metadata": {},
   "source": [
    "The Support vector machine algorithm (SVM) aims to categorize points, of different nature, located in a given space. The SVM achieves that by finding a frontier that separates groups of points by the category they represent.\n",
    "\n",
    "For the binomial case (Two possible categories), in a 2-dimensional space (two components), the points represent two independent variables, and a line exemplifies the frontier (Figure 1).\n",
    "\n",
    "<img src=\"Fig1.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 1</p>\n",
    "\n",
    "For example, we could set environmental pressure and temperature as independent variables and the state rain or no rain as independent variables. If we have found an optimum 2D frontier, the model would probably tell us that having high environmental pressure and low-temperature could cause rain. Another part of the space would contain the position of the points that do not cause rain.\n",
    "\n",
    "Since this example is oversimplified and very likely non-real, we could use more independent variables to find a better representation.\n",
    "\n",
    "In the same way that we use a line in a 2D space, we can use a point to separate a 1D space (Figure 2), a plane in a 3D space, and a hyperplane in a higher-dimensional space.\n",
    "\n",
    "<img src=\"Fig2.png\" width=500 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 2</p>\n",
    "\n",
    "Coming back to the 2D example, we can say that there is an infinite number of lines that can separate two categories of points (Figure 3). The line that is further apart from the closest point of the two classes is the one that generalizes the better.\n",
    "\n",
    "Therefore, the line that the algorithm chooses must accomplish two things, separate the set of points as correct as possible, and create the widest margin. The margin is defined as the distance between the model and the closest point to it.\n",
    "\n",
    "\n",
    "<img src=\"Fig3.png\" width=500 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 3</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aff08c",
   "metadata": {},
   "source": [
    "# SVM hard margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ddc0b1",
   "metadata": {},
   "source": [
    "A plane in a 2D space can be defined by $\\omega_1 x_1 + \\omega_2 x_2 + b= 0$, where each ith $\\omega$ component is multiplied by the $x$ component with the same index. Therefore, the same equation can be written as $\\omega^T x + b = 0$, which maintain the same form for higher dimensions.\n",
    "\n",
    "Now we define the same scenario for general dimension. We can define a function that transform a vector from a space with dimension $D$ to the real numbers: $R^D \\rightarrow{} R$; $x \\rightarrow f(x)$. Mapping the whole hyperplane (a line in 2D). would return in all cases $$f(x) = \\omega^T x + b = 0   \\; \\; (1)$$\n",
    "\n",
    "We want to find a hyperplane, for which we get $f(x_k) = \\omega^T x_k + b \\geq 0$ when the example $x_k$ falls on the positive side of the hyperplane and $f(x_k) = \\omega^T x_k + b < 0$ on the negative side.\n",
    "\n",
    "If we define $x_k$ as a point and $\\omega$ as a vector, we can find that $\\omega$ is perpendicular to the hyperplane over the function $f(x)$. \n",
    "We can choose two points $x_a, x_b$ living inside the plane. Therefore,\n",
    "\n",
    "$$f(x_a) - f(x_b) = (\\omega^T x_a + b) - (\\omega^T x_b + b)$$\n",
    "$$= \\omega^T x_a - \\omega^T x_b -b + b$$\n",
    "\n",
    "by the linearity of the dot product, and remembering that the points live inside the plane ($f(x) = 0$)\n",
    "\n",
    "$$f(x_a) - f(x_b) = \\omega^T(x_a - x_b) = 0$$\n",
    "\n",
    "from here we can see that the dot product of $\\omega$ and the vector created by $x_a - x_b$ is equal to zero, which means they are linearly independent, or orthogonal to each other.\n",
    "\n",
    "<img src=\"Fig4.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 4</p>\n",
    "\n",
    "We have found that the vector $\\omega$ is perpendicular to the plane, and therefore, we can define the distance to any point as.\n",
    "\n",
    "$$d_H(x_k) =  \\frac{|\\omega^Tx_k + b|}{||\\omega||_2} $$\n",
    "\n",
    "We are trying to find a hyperplane that maximizes the margin (distance from the plane to the closes point), and therefore, we must find the closest positive and negative examples.\n",
    "\n",
    "$$ \\min_k d_H(x_k) = \\frac{|\\omega^T x_k + b|}{||\\omega||_2}$$\n",
    "\n",
    "And then, find the longest distance between these points. The parameters that manipulate the position of the hyperplane are $\\omega$ and $b$, thus\n",
    "\n",
    "$$ \\max_{w, b} \\left \\{ \\min_k d_H(x_k) = \\frac{|\\omega^T x_k + b|}{||\\omega||_2} \\right \\} \\;\\;\\;\\;\\;(2)$$\n",
    "\n",
    "Remembering that we want to find a function that classifies as positive, when the label $y_k$ is equal to 1 and negative, when the label is equal to -1,\t\t\n",
    "\n",
    "$$f(x_k) = \\omega^T x_k + b \\geq 0 \\;\\;\\;\\; y_k = 1$$ \n",
    "$$f(x_k) = \\omega^T x_k + b < 0 \\;\\;\\;\\; y_k = -1$$\n",
    "\n",
    "We can compress the two previous equations, in the form\n",
    "$$y_k[\\omega^Tx_k + b] \\geq 0 \\;\\;\\;\\; (3)\n",
    "$$\n",
    "\n",
    "where the previous inequality is satisfied when the example is correctly classified.\n",
    "\n",
    "Since (3) should be positive defined for all examples, the following equality stands, $y_k[\\omega^Tx_k + b] = |\\omega^T x_k + b|$, and we can substitute (3) in (2).\n",
    "\n",
    "$$ \\max_{w, b} \\left \\{ \\min_k \\frac{y_k[\\omega^Tx_k + b] }{||\\omega||_2} \\right \\} $$\n",
    "\n",
    "Taking $||\\omega||_2$ out of the equation since it does not depend on $k$\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "&\\max_{w, b} \\frac{1}{||\\omega||_2} \\left \\{ \\min_k y_k[\\omega^Tx_k + b] \\right \\} & (4)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Currently we don't what is the distance from the plane to the closes point. A quantity $r$ can be defined as it, and therefore, we can say that the equation $y_k[\\omega^Tx_k + b] = r$ is satisfied when $k$ is minimum.\n",
    "\n",
    "We can manipulate (4) as two different entities, maximizing $\\frac{1}{||\\omega||_2}$, given the constraint $\\min_k y_k[\\omega^Tx_k + b] = r$.\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\max_{\\omega, b} \\frac{1}{||\\omega||_2}\\\\\n",
    "\\text{S. t.   } \\min_{k} y_k[\\omega^T x_k + b] = r & (5)\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Stating that the rest of points are at a distance greater than $r$. \n",
    "\n",
    "<img src=\"Fig5.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 5</p>\n",
    "\n",
    "If we want to normalize the distance from the closes point, so that it becomes 1 instead of $r$, we can first define the following transformation.\n",
    "$$ \\omega = r\\omega' \\text{ and } b = rb'$$\n",
    "\n",
    "$$y_k[(r\\omega)^T x_k + rb] = r$$\n",
    "\n",
    "And divide both sides by $r$\n",
    "\n",
    "$$ y_k[(\\omega)^T x_k + b] = 1 $$\n",
    "\n",
    "The given constraint is then given by \n",
    "\n",
    "$$ \\min_{k} y_k[(\\omega)^T x_k + b] = 1$$\n",
    "\n",
    "Which tell the algorithm to find certain $\\omega$ and $b$ so that the distance of $x_{closest}$ is equal to 1, which is the same as saying that all values have to be equal or grater than 1.\n",
    "\n",
    "$$\\min_{k} y_k[(\\omega)^T x_k + b] = 1 \\longrightarrow y_k[(\\omega)^T x_k + b] \\geq 1$$\n",
    "\n",
    "And therefore, we obtain from (5)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\max_{\\omega, b} \\frac{1}{||\\omega||_2}\\\\\n",
    "&\\text{S. t.  } y_k[\\omega^T x_k + b] \\geq 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Instead of maximizing the function $\\frac{1}{||\\omega||_2}$, it is usual to minimize the square of the reciprocal and multiply it by a constant $\\frac{1}{2}$ to get a cleaner solution when the gradient is computed.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\min_{\\omega, b} \\frac{1}{2} ||\\omega||_2^2\\\\\n",
    "&\\text{S. t.  } y_k[\\omega^T x_k + b] \\geq 1 & (6)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The previous equation can be solved using Lagrange multipliers, or convex optimization packages. The result of this two, would return a vector $omega$ and $b$, which will define a hyperplane that would predict positive values as positive, and negative values as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130e610",
   "metadata": {},
   "source": [
    "# Slack Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5bf8a",
   "metadata": {},
   "source": [
    "Real life datasets very rarely are separable, and thus the hard margin cannot solve this kind of problems.\n",
    "\n",
    "<img src=\"Fig6.png\" width=500 height=400 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 6</p>\n",
    "\n",
    "To solve this issue the slack variable ($\\xi$) is introduced, which allows for some error when creating the hyperplane.\n",
    "\n",
    "Lets define the margin of the positive examples as $m+$ and $m-$ for the opposite margin, and $+x_k$ for examples with label $y = 1$ and $-x_k$ for $y = -1$. Therefore the slack variable is given by the difference between $\\pm m$ and $\\pm x_k $ (Figure 7). With the constraint that this measurement just apply to examples that are inside the margin or on the wrong side of the hyperplane.\n",
    "\n",
    "<img src=\"Fig7.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 7</p>\n",
    "\n",
    "Instead of (6) we use the following equation to find the optimum model\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\min_{\\xi, \\omega, b} \\sum_{k = 1}^N \\xi_k & (7.1)\\\\\n",
    "& \\xi_k = \\max \\left(0, 1 - y_k[\\omega_k^T x_k + b]  \\right) & (7.2)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "lets define \n",
    "$$\n",
    "\\begin{align}\n",
    "& t = y_k[\\omega_k^T x_k + b]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "There are two main scenarios involving the previous model in a given space:\n",
    "\n",
    "- If the point lay in the correct side of the hyperplane, $t$ will get a value grater than 1. The max function, will return the maximum value, which is 0, and its contribution to the total error is null.\n",
    "\n",
    "\n",
    "- If the point lay either inside the margin or on the wrong side of the hyperplane, $t$ will return a value smaller than 1. The max function will return a value greater than 0, contributing to an increase in the total error ($\\xi$).\n",
    "\n",
    "<font size=\"2\"> (7.2) is known as the hinge loss function. </font> \n",
    "\n",
    "Another way of expressing the Hinge loss\n",
    "\n",
    "$$\n",
    "\\xi_k = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "0 & t \\geq 1\\\\\n",
    "1-t & t < 1\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "An optimum model will get a value of $\\sum_{k = 1}^N \\xi_k = 0$. Since this is not possible for non-separable datasets, (7.1) will find a minimum when most cases are correctly classified. \n",
    "\n",
    "We can express (7.2) as $\\xi \\geq 0$ and $\\xi \\geq 1 - t$. The first one selects from the set of examples and measure the non-correctly classified. The latter allow for some error, and relates it with the optimization coefficients. \n",
    "\n",
    "Therefore (7) can also be written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\min_{\\xi, \\omega, b} \\sum_{k = 1}^N \\xi_k &\\\\\n",
    "& \\text{S. t. } y_k[\\omega_k^T x_k + b] \\geq 1 - \\xi_k & (8)\\\\\n",
    "& \\xi_k \\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$ \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    \\omega^T x_k + b \\geq 1 - \\xi_k & y_k = 1\\\\\n",
    "    \\omega^T x_k + b \\leq -1 + \\xi_k & y_k = -1\\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eace985",
   "metadata": {},
   "source": [
    "# SVM soft margin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319e5c10",
   "metadata": {},
   "source": [
    "The previous algorithm is a better approach for non-separable datasets, but it fails on creating the largest margin, on separable datasets. Therefore, a combination of both models was proposed. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\min_{\\xi, \\omega, b} \\frac{1}{2} ||\\omega||^2 + C\\sum_{k = i}^N \\xi_k\\\\\n",
    "& \\text{S. t.  } y_k[\\omega^T x_k + b] \\geq 1 - \\xi & (8)\\\\\n",
    "\\\\\n",
    "& \\xi_k \\geq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This model will try to maximize the length of the margin, allowing some points to fall on the wrong side of the hyperplane.\n",
    "\n",
    "\n",
    "Comparing this equation with the regularized linear regression loss function, we can see that for this case, the model tries to find a hyperplane that classifies the majority of points correctly ($C\\sum_{k = i}^N \\xi_k$), and to not over-fit, it tries to find the largest margin between the points ($\\frac{1}{2} ||\\omega||^2$).\n",
    "\n",
    "The last term $\\xi_k \\geq 0$, which is given by its definition (7.2),  is explained in the previous paragraph.\n",
    "\n",
    "The $C$ parameter regulates the impact of the examples lying on the wrong side. If we choose $C=\\infty$ the penalty would be infinite if at least one example lies on the wrong side. The model would not allow values on the wrong side. \n",
    "In the contrary, if we choose $C = 0$ then any example falling in the margin or in the opposite site would not penalize the function, and therefore all points would contribute to create the margin.\n",
    "\n",
    "from the previous paragraph we can derive that the largest the value of C, the more regularized it is.\n",
    "\n",
    "<img src=\"Fig8.png\" width=600 height=600 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 8</p>\n",
    "\n",
    "Slack variable explanation\n",
    "- https://www.quora.com/What-is-the-purpose-for-using-slack-variable-in-SVM\n",
    "\n",
    "### Lagrange multipliers solution\n",
    "\n",
    "We can solve the previous equation by the Lagrange multipliers method. We will solve it by the dual problem. As mention in the appendix, one advantage of solving by the dual Lagrangian is passes from depending on primal variables to dual variables, therefore, allowing the use of spaces with infinite dimensions (feature spaces). Therefore, from now on, we will use the inner product notation, instead of the more specific dot product notation $x^Tx \\rightarrow \\langle x, x \\rangle$.\n",
    "\n",
    "First we set up the Lagrangian. Instead of using the commonly used notation for Lagrange multiplier ($\\lambda$), we use $\\alpha$ and $\\eta$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\mathcal{L}(\\omega, b, \\xi, \\alpha, \\eta) = \\frac{1}{2}||\\omega||^2 + C\\sum_{k = 1}^N \\xi_k - \\sum_{k = 1}^N \\alpha_k \\left( y_k[\\langle \\omega, x_k \\rangle + b]- 1 + \\xi_k \\right) - \\sum_{k = 1}^N \\eta_k \\xi_k & (9)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $\\alpha \\geq 0$ corresponding to examples correctly classified and $\\eta \\geq 0$ to positive error values\n",
    "\n",
    "Then we derive over the three primal variables $\\omega$, $b$ and $\\xi$ and equalize to zero (first order conditions).\n",
    "​\n",
    "- $\\nabla_{\\omega}$\n",
    "​\n",
    "$||\\omega|| = \\sqrt{\\langle \\omega, \\omega \\rangle}$\n",
    "​\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\therefore \\nabla_{\\omega}\\mathcal{L} = \\omega - \\sum_{k = 1}^N \\alpha_k y_k x_k = 0 \\longrightarrow \\omega = \\sum_{k = 1}^N \\alpha_k y_k x_k \\;\\;\\;\\;\\;\\ (10a)\n",
    "\\end{equation}\n",
    "$$\n",
    "​\n",
    "- $\\frac{\\partial\\mathcal{L}}{\\partial b}$\n",
    "​\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial b} = \\sum_{k = 1}^N \\alpha_k y_k = 0 \\longrightarrow \\sum_{k = 1}^N \\alpha_k y_k = 0 \\;\\;\\;\\;\\;\\ (10b)\n",
    "\\end{equation}\n",
    "$$\n",
    "​\n",
    "- $\\frac{\\partial\\mathcal{L}}{\\partial \\xi_k}$\n",
    "​\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial\\mathcal{L}}{\\partial \\xi_k} = C - \\alpha_k - \\eta_k = 0 \\longrightarrow C - \\alpha_k = \\eta_k \\;\\;\\;\\;\\;\\ (10c_a)\n",
    "\\end{equation}\n",
    "$$\n",
    "​\n",
    "Since $\\alpha_k, \\eta_k \\geq 0$\n",
    "​\n",
    "$$ C -\\alpha_k = +$$\n",
    "​\n",
    "We get that \n",
    "​\n",
    "$$ 0 \\leq \\alpha_k \\geq C \\;\\;\\;\\;\\;\\ (10c_b)$$\n",
    "\n",
    "Substituting (10a) and (10b) in (9) we get the dual problem\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{D}(\\alpha, \\eta) = \n",
    "\\frac{1}{2}\\sum_{k = 1}^N \\alpha_k y_k x_k \\left (\\sum_{l = 1}^N \\alpha_l y_l x_l \\right) + \n",
    "C\\sum_{k = 1}^N \\xi_k -\n",
    "\\sum_{k = 1}^N \\alpha_k y_k \\left( \\sum_{l = 1}^N \\alpha_l y_l x_l \\right) x_k - \n",
    "\\sum_{k = 1}^N \\alpha_k y_k b + \n",
    "\\sum_{k = 1}^N \\alpha_k - \n",
    "\\sum_{k = 1}^N \\alpha_k \\xi_k -\n",
    "\\sum_{k = 1}^N \\eta_k \\xi_k \\;\\;\\;\\;\\; (11)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "- The first term is a composition of two sums. Since the index $k$ is independent of $l$, the coefficients can be combined following each sum order\n",
    "\n",
    "- From the third term we know that the inner product is symmetric and the process of the first term also applies.\n",
    "\n",
    "- In the fourth coefficient the constant b can be taken outside of the sum, and from (10b) we know that the sum is equal to 0.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{D}(\\alpha, \\eta) = \n",
    "\\frac{1}{2}\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l +\n",
    "C\\sum_{k = 1}^N \\xi_k -\n",
    "\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l +\n",
    "\\sum_{k = 1}^N \\alpha_k -\n",
    "\\sum_{k = 1}^N \\alpha_k \\xi_k -\n",
    "\\sum_{k = 1}^N \\eta_k \\xi_k \\;\\;\\;\\;\\; (12)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "- The first term is 1/2 of the third term.\n",
    "\n",
    "- The second, fifth and sixth terms have the vector $\\xi_k$ in common and the sum runs on the same index.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{D}(\\alpha, \\eta) = \n",
    "-\\frac{1}{2}\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l +\n",
    "\\sum_{k = 1}^N \\alpha_k +\n",
    "\\sum_{k = 1}^N \\left(C  -\\eta_k - \\alpha_k \\right) \\xi_k \\;\\;\\;\\;\\; (13)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The last term has the form of ($10c_a$), where each kih term is equal to 0, and therefore, each product is equal to 0 and the term fades away.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{D}(\\alpha, \\eta) = \n",
    "-\\frac{1}{2}\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l +\n",
    "\\sum_{k = 1}^N \\alpha_k \\;\\;\\;\\;\\; (14)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Recalling that in the dual problem  $\\mathcal{D}(\\alpha, \\eta)$ we want to maximize over the dual variables (appendix dual problem), and (10b).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\max_{\\alpha, \\eta} -\\frac{1}{2}\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l +\n",
    "\\sum_{k = 1}^N \\alpha_k\\\\\n",
    "& \\text{S. t. }  \\sum_{k = 1}^N \\alpha_k y_k = 0\\\\\n",
    "& 0 \\leq \\alpha \\geq C\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We know that the previous function is either concave or convex. We can compute the minimum of the negative of the resulting problem\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\min_{\\alpha, \\eta} \\frac{1}{2}\\sum_{k = 1}^N \\sum_{l = 1}^N \\alpha_k \\alpha_l y_k y_l x_k x_l -\n",
    "\\sum_{k = 1}^N \\alpha_k\\\\\n",
    "& \\text{S. t. }  \\sum_{k = 1}^N \\alpha_k y_k = 0\\\\\n",
    "& 0 \\leq \\alpha \\geq C\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Its necessary to apply complementary Slackness Constraints, which in this case is given by.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    &\\alpha_k(y_k[\\langle \\omega, x \\rangle + b] - 1 + \\xi_k) = 0 & (15a)\\\\\n",
    "    &\\eta_k \\xi_k = 0 & (15b)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since the previous product is equal to zero and recalling $\\alpha_k, \\eta_k \\geq 0$, there are two posibilidades for each equality.\n",
    "\n",
    "$(15a)$\n",
    "\n",
    "\\# 1 $\\alpha_k = 0$ and $y_k[\\langle \\omega, x \\rangle + b] \\geq 1 - \\xi_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f9d74",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f16b2",
   "metadata": {},
   "source": [
    "The problem above can be solve numerically with a quadratic solver, which for the dual problem return the set of optimum $\\alpha_k$  that satisfy the previous conditions.\n",
    "\n",
    "The problem is to find the optimum hyperplane that separate the dataset and gets the largest margin. For the **linear kernel** problem we can find $\\omega$ and $b$ values from the optimum $\\alpha$. In other words, due to the Complementary Slackness Condition, the number of $\\alpha_k$ and therefore, the number of variables used to find optimum primal parameters is reduced. To predict we can use the previously found coefficients and insert them on the plane equation with the vector of independent variables to predict ($x_test$).\n",
    "\n",
    "Given\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    f(x) =  \\omega^T x+ b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Finding optimum values**\n",
    "\n",
    "- $\\omega$ from (10a)\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\omega = \\sum_{k = 1}^N \\alpha_k y_k x_k = (\\mathbf{\\alpha} \\circ \\mathbf{y}) \\circ \\mathbf{x}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "As shown in the previous equation, the $\\omega$ coefficient can be numerically computed by element-wise multiplication between vectors.\n",
    "\n",
    "- From $y_k[\\omega^T x_k + b] = 1$ we can solve for $b$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    b' = \\frac{1}{y_l} + \\omega^T x_l = y_l + \\omega^T x_l = y_l + \\sum_{k = 1}^N \\alpha_k y_k x_k^T x_l\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "When finding optimum coefficients the $k$ index represents the filtered variables, while the $l$ index represent the non-filtered variables.\n",
    "\n",
    "The $b$ coefficient must be a constant. Since the previous equation returns a vector. The mean of the previous vector is computed.\n",
    "\n",
    "$$ b = \\frac{1}{N}\\sum_{k = 1}^N(b') $$\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    f(x) =  \\omega^T x_{\\text{test}} + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For the **non-linear and non-separable kernels**, it is not possible to find an $\\omega$, instead $\\langle \\omega, \\phi \\rangle$, is used which is dependent on the kernel (is a computable coefficient) and can be inserted in the plane equation.\n",
    "To predict new values, a new kernel in terms of the training and test examples must be constructed  ($k(x_k, x_{\\text{test}})$).\n",
    "\n",
    "Given\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    f(x) = \\langle \\omega, \\phi(x) \\rangle + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Finding optimum coefficient**\n",
    "\n",
    "Recalling the slackness condition, the Kernel is filtered (as well as the rest of variables), corresponding to the remaining $\\alpha_k$. The only coefficient that we need for this case is $b$, which will be permanent even for new values. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    b = \n",
    "    \\frac{1}{N}\\sum_{l = 1}^N \\left[ y_l + \\langle \\omega, \\phi(x_l) \\rangle \\right] = \n",
    "    \\frac{1}{N}\\sum_{l = 1}^N \\left[ y_l +  \\left( \\sum_{k = 1}^N \\alpha_k y_k \\phi(x_k)\\right)\\phi(x_l) \\right] =\n",
    "    \\frac{1}{N}\\sum_{l = 1}^N \\left[ y_l +  \\sum_{k = 1}^N \\alpha_k y_k K(x_k, x_l) \\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$x_l$: non_filtered vectors\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "As said before, when predicting, new kernels have to be constructed with the new input variables.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\langle \\omega, \\phi_{\\text{test}} \\rangle = \n",
    "    \\left( \\sum_{k = 1}^N \\alpha_k y_k \\phi(x_k)\\right)\\phi(x_{\\text{test}}) = \n",
    "    \\sum_{k = 1}^N \\alpha_k y_k K(x_k, x_{\\text{test}})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "And finally, those values have to be inserted in the plane equation\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    f(x) = \\langle \\omega, \\phi_{\\text{test}} \\rangle + b\n",
    "\\end{equation}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a532b2f",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada2a73",
   "metadata": {},
   "source": [
    "### Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948625f",
   "metadata": {},
   "source": [
    "The classic representation of a geometrical vector in a 2D space is given by $y = xm + b$. Where m represents the slope of the line and b the intercept. Also $-y + xm + b = 0 \\longleftrightarrow{} -f(x) + xm + b = 0$. A more general representation can be given by $a_1 \\hat{i} + a_2 \\hat{j} + b= 0$, where $\\hat{i}$ and $\\hat{j}$ are the unitary vectors. In an euclidean space, the same vector can be represented as $a_1 \\hat{e_1} + a_2 \\hat{e_2} + b = 0$. If we decompose $a_i = q_i \\cdot \\lambda$, where $a_i$ represents a constant ($q_i$) multiplied by any real value ($\\lambda$), we can rewrite the same vector as. $(q_1 \\cdot \\lambda) \\hat{e_1} + (q_2 \\cdot \\lambda) \\hat{e_2} + b = 0$. That means that the same value of $\\lambda$ is assigned to each component for a given point; therefore, having just one free variable.\n",
    "\n",
    "Using the last notation, we can define a point in a 2D space. For example: $$4\\hat{e_1} + 2\\hat{e_2} + 5 = 0$$ \n",
    "Where $a_1 = 2 \\cdot 2$,  $a_2 = 1 \\cdot 2$ and $b = 5$. \n",
    "\n",
    "For this vector $q_1$ always takes the value of 1,while $q_2$ is always 2\n",
    "\n",
    "Another point inside the same vector, can be defined as $3\\hat{e_1} + 2\\hat{e_2} + 5 = 0$, which can, at the same time, span the whole vector/subspace/affine subspace, due to the vector $\\leftrightarrow{}$ point duality.\n",
    "\n",
    "<img src=\"Fig9.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 9</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d15283",
   "metadata": {},
   "source": [
    "### Plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7373fb6",
   "metadata": {},
   "source": [
    "A plane inside a 3D space can be defined as $Ax + By + Cz = D \\leftrightarrow{} Ax + By + Cz - D = 0$  in generalized notation, it can be represented as $a_1 \\hat{e_1} + a_2 \\hat{e_2} + a_3 \\hat{e_3} = a_4$\n",
    "\n",
    "In the same way, each component can be decompose into constants and incremental variables. For this case, we would need two free variable as follows $a_i = (p_i \\cdot \\lambda + q_i \\cdot \\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430a459",
   "metadata": {},
   "source": [
    "### Distance from a plane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4352d7",
   "metadata": {},
   "source": [
    "####  1st derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74602",
   "metadata": {},
   "source": [
    "Given a euclidean space (where the components of the basis are orthonormal), we can find a distance between a point outside the plane and the plane by finding the length of any vector perpendicular to the plane, which will guarantee the minimum distance.\n",
    "\n",
    "In this case, we first need to define any point in the plane and create a vector from this point to the point in question. $\\vec{a}$ will be created.\n",
    "\n",
    "Then, we will construct a vector $\\vec{b}$ perpendicular to the plane (Figure 5). It can be proven that a normal vector can be constructed with the value of the components of the plane (e.g $4x + 3y + 3 = 0  \\rightarrow{} \\vec{v} = 4\\hat{i} + 3\\hat{j}$)\n",
    "\n",
    "The dot product between these two vectors will return the length of $\\vec{a}$ in the direction of $\\vec{b}$ or the length of $\\vec{b}$ in the direction of $\\vec{a}$. Since we are interested in the \n",
    "\n",
    "In order to get the projection of $\\vec{b}$ in the direction of $\\vec{a}$, we can calculate the dot product. The dot product returns a combination of the two vectors components. Since we are only interested on the length of the vector $\\vec{a}$, we will dot the vector $\\vec{a}$ with the unitary vector $\\frac{\\vec{b}}{||b||}$, and return the absolute value as we are looking for the distance.\n",
    "\n",
    "$$dist(a, P_0) = \\frac{|a \\cdot b|}{||b||_2}$$\n",
    "\n",
    "<font size=\"2\">* Remembering the point-vector duality, we can define the given point as a vector. </font> \n",
    "\n",
    "https://www.youtube.com/watch?v=SgJo7_4mp6w\n",
    "\n",
    "<img src=\"Fig10.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 10</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aad2d6",
   "metadata": {},
   "source": [
    "#### Second derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2b0da",
   "metadata": {},
   "source": [
    "From Figure 10 we can see that the distance of the point $a$ to the plane is given by $distance(a, P_o) = ||b||\\cos{\\theta}$. \n",
    "\n",
    "We haven't define a value for the angle between the two vectors, but we know that the dot product between two vectors $a$ and $b$ is defined as $a \\cdot b = ||a||||b||\\cos{\\theta}$. Solving for $||b||\\cos{\\theta}$ we get, $||b||\\cos{\\theta} = \\frac{a \\cdot b}{||b||}$.\n",
    "\n",
    "Since the dot product returns positive and negative values, depending of the value of the angle between the two vectors, we calculate the absolute value of the previous product. For this case, we will used the norm 2* to compute the vectors length.\n",
    "\n",
    "$$ dist(a, P_0) = \\frac{|a \\cdot b|}{||b||_2}$$\n",
    "\n",
    "For hyperplanes in non-euclidean spaces\n",
    "\n",
    "$$ dist(a, P_0) =  \\frac{|\\langle a, b \\rangle|}{\\sqrt{\\langle b, b \\rangle}}$$\n",
    "\n",
    "<font size=\"1\">* A vector $\\vec{v}$ norm 2 is defined as $\\sqrt{\\sum_{i=1}^N v_i}$. </font> \n",
    "\n",
    "If our plane is defined as $Ax + By + Cz + d = 0$ and the point has coordinates from the origin $(x_l, y_l, z_l)$ \n",
    "$$ dist(a, P_o) =  \\frac{|Ax_l + By_l + Cz_l - (Ax_0 + By_0 + Cx_0)|}{\\sqrt{A + B + C}}$$\n",
    "\n",
    "then the distance can be defined as:\n",
    "\n",
    "$$ dist(a, P_o) =  \\frac{|Ax_l + By_l + Cz_l + d|}{\\sqrt{A + B + C}} $$\n",
    "\n",
    "<font size=\"1\">* Notice that the intercept was added due to the relative position of the vector $\\vec{a}$ from the plane. </font> \n",
    "\n",
    "https://youtu.be/zWMTTRJ0l4w\n",
    "\n",
    "<img src=\"Fig11.png\" width=500 height=500 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 11</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f683feb",
   "metadata": {},
   "source": [
    "#### Constrained problems\n",
    "\n",
    "**Lagrange Multiplier**\n",
    "\n",
    "The Lagrange multipliers states that there is certain point of a constraint g(x) and a function f(x) where the tangent passing through both functions, will have the same direction, and therefore their gradients would be equivalent.\n",
    "\n",
    "<img src=\"Fig12.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 12</p>\n",
    "\n",
    "It can be described mathematically as\n",
    "\n",
    "$$ \\nabla f(x) = \\lambda \\nabla g(x)$$\n",
    "\n",
    "$$ \\nabla f(x) - \\nabla g(x) = 0$$\n",
    "\n",
    "$$ \\nabla[f(x) - g(x)] = 0$$\n",
    "\n",
    "We can define an auxiliary function\n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda) = f(x) - \\lambda g(x)$$\n",
    "\n",
    "For multiple constraints\n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda) = f(x) - \\sum_{i = 1}^{N}\\lambda_i g(x) = f(x) - \\lambda^T g(x)$$\n",
    "\n",
    "The gradient over $x$ and $\\lambda$ is obtained and equalized to zero to find the local (as the function is constrained) maximum and minimum.\n",
    "\n",
    "It is important to mention that the sign of the Lagrangian's second coefficient as shown on (). can be either positive or negative, in which case, the $\\lambda$ can take either positive or negative values to compensate.\n",
    "\n",
    "The Lagrange multipliers can solve for equality constraints ($h(x) = c$) as well as inequalities ($g(x) \\leq 0$). For the latter, the signs of the coefficient is important, the Lagrange Multiplier has to be constrained whether a minimization or a maximization is needed (see KKT)\n",
    "$g(x)>0$ or $g(x)<0$. It also has to satisfy the complementary slackness condition. This is explained in-depth in the Karush-Kuhn-Tucker condition section.\n",
    "\n",
    "Lagrange multipliers\n",
    "- https://www.youtube.com/watch?v=8mjcnxGMwFo\n",
    "\n",
    "- https://towardsdatascience.com/lagrange-multiplier-demystified-1e5fd55cee5a\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Lagrange_multiplier#:~:text=In%20mathematical%20optimization%2C%20the%20method,chosen%20values%20of%20the%20variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358374c8",
   "metadata": {},
   "source": [
    "#### Karush–Kuhn–Tucker conditions\n",
    "\n",
    "The Karush-Kuhn-Tucker (KKT) conditions is a set of rules to follow when solving for general optimization problems by the Primal and Dual Lagrangian. \n",
    "\n",
    "Given a target function to optimize $f(x)$ and constraints $g_i(x) \\leq 0$ and $h_j(x) = 0$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{opt} \\;\\; & f(\\mathbf{x})\\\\\n",
    "& \\mathbf{g}(\\mathbf{x}) \\leq 0\\\\\n",
    "& \\mathbf{h}(\\mathbf{x}) = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The Lagrangian is defined as\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "    \\mathcal{L(x, \\lambda, \\mu)} = f(\\mathbf{x}) + \\lambda^T\\mathbf{g}(\\mathbf{x}) + \\mu^T\\mathbf{h(x)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Given $\\lambda \\geq 0, g(x) \\geq 0$\n",
    "\n",
    "$-[(+)(+)] = (-) \\rightarrow$ minimizing, negative gradient\n",
    "\n",
    "$-[(+)(-)] = (+) \\rightarrow$ minimizing, constraint $g(x) \\geq 0$ broken, positive gradient\n",
    "\n",
    "Karush–Kuhn–Tucker conditions\n",
    "- https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions\n",
    "\n",
    "#### Indicator Function\n",
    "\n",
    "We can solve a constrained problem of the form \n",
    "$$ \n",
    "\\begin{align}\n",
    "&\\min_{x} f(x)\\\\\n",
    "& & (a)\\\\\n",
    "&\\text{S. t.  } g(x) \\leq 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Transforming it into an unconstrained problem through an indicator/characteristic function\n",
    "\n",
    "$\\mathcal{P}(x) = f(x) + \\sum_{i = 1}^N \\mathbf{1} g(x) $\n",
    "\n",
    "where \n",
    "\n",
    "\n",
    "$$ \\mathbf{1}(\\mathcal{z})=  \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 & \\mathcal(z) \\leq 0 \\\\\n",
    "      \\infty & \\mathcal(z) > 0\\\\\n",
    "\\end{array} \n",
    "\\right. $$ \n",
    "\n",
    "\n",
    "Which gives infinity penalty when the constriction is not satisfied. In most cases this method is not easy to calculate.\n",
    "The Lagrange multipliers is used to solve this kind of problems instead.\n",
    "\n",
    "#### Duality \n",
    "\n",
    "The dual problem in Lagrange multipliers is used to simplify the computation of some sets, it also helps on finding extra information about the primal problem, which helps to find a solution.\n",
    "\n",
    "\n",
    "In the case of the Support Vector Machine, it helps to insert kernels into the problem, which can only be added when the loss function does not depend on the primal variables.\n",
    "\n",
    "The primal problem solve for n number of variables and m constraints, while the dual problem solves for m variables and n constraints.\n",
    "\n",
    "Given the standard constrained problem, we know it can be solved by constructing the Lagrangian, which depends on two variables $x$ and $\\lambda$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(x, \\lambda) = f(x) + \\sum_{i = 1}^{N}\\lambda_i g(x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last equation, if we let lambdas get any real value, and the constraint ($g(x) \\leq 0$) is broken, the model can find an opposite sign $\\lambda$, that minimizes $f(x)$ over $x$ by the product $-\\lambda\\cdot + g(x)$, then letting the model brake many constraints.\n",
    "\n",
    "If we want to minimize $f(x)$ and at the same time, brake as few conditions as possible. The problem must be constraint for lambdas. Now we allow for positive values of $\\lambda$ and try to maximize its value, so when a condition is broken, the maximum penalty is applied.\n",
    "\n",
    "We can associate the problem with the indicator function, where we get 0 when $\\mathbf{1}(\\mathcal{z}) \\leq 0$ ($\\lambda \\geq 0$) and $\\infty$ when $\\mathbf{1}(\\mathcal{z}) \\leq 0$ ($\\max_{\\lambda \\geq 0}$). Thus, when $\\lambda \\geq 0$, the Lagrangian is a lower bound of the $\\mathcal{P(x)}$ function.\n",
    "\n",
    "$$\\mathcal{P(x)} = \\max_{\\lambda \\geq 0} \\mathcal{L(x, \\lambda)}$$\n",
    "\n",
    "Ultimately what we want is to minimize $\\mathcal{P(x)}$ over $x$, therefore, we obtain\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{x} \\mathcal{P(x)} = \\min_{x} \\max_{\\lambda \\geq 0} \\mathcal{L(x, \\lambda)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In other words, the minimum over $x$ of the primal problem is equal to the minimum over $x$ of the maximum over $\\lambda\\geq 0$ of the Lagrangian.\n",
    "\n",
    "$\\mathcal{P(x)}$ is known as the **primal** function. which we can use to solve sets in the form ($a$). We could try to solve in the opposite way, specifically \n",
    "$$\\max_{\\lambda \\geq 0} \\min_{x} \\mathcal{L(x, \\lambda)}$$\n",
    "\n",
    "Where \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{D(\\lambda)} =  \\min_{x} \\mathcal{L(x, \\lambda)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\mathcal{D(\\lambda)}$ is known as the **dual** function and $\\lambda$ the dual variable.\n",
    "\n",
    "But it turn out that swapping the order of the pair min-max does not return the same value for all cases. This is known as the min-max inequality.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{x} \\max_{\\lambda \\geq 0} \\mathcal{L(x, \\lambda)} \\leq \n",
    "\\max_{\\lambda \\geq 0} \\min_{x} \\mathcal{L(x, \\lambda)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "There is a special group of function for which the min-max of the Lagrangian return the same value as the min-max. This group of functions is called convex functions. \n",
    "\n",
    "The SVM target function and constraints are all convex, and therefore, the min-max inequality does not have an effect on this problem.\n",
    "\n",
    "1. The dual problem is constructed by first building the Lagrangian of a minimization problem ($\\min_{x} \\mathcal{L(x, \\lambda)}$), with positive defined dual variables ($\\lambda \\geq 0$). \n",
    "2. Apply the first order conditions ($\\nabla \\mathcal{L}(x) = 0$) with respect to the primal variables. \n",
    "3. Rebuild the Lagrangian with respect to the dual variables and add the constraints that resulted from the previous step.\n",
    "\n",
    "<img src=\"Fig13.png\" width=300 height=300 />\n",
    "\n",
    "<p style=\"text-align: center;\">Figure 13</p>\n",
    "\n",
    "Duality and Complementary Slackness Condition\n",
    "\n",
    "https://courses.cs.washington.edu/courses/cse521/16sp/521-lecture-16.pdf\n",
    "\n",
    "Convex optimization, min-max inequality and Linear Programming\n",
    "- https://youtu.be/oLowhs83aHk\n",
    "\n",
    "Convex function\n",
    "- https://youtu.be/nOFXLCCvtm0\n",
    "\n",
    "##### Complementary Slackness\n",
    "\n",
    "The complementary Slackness constraints are constraints necessary to solve a dual problem and help on finding the points that work as support vectors on the SVM problem. \n",
    "\n",
    "They are the product of the dual and primal variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
